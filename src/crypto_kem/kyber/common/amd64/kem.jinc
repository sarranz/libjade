// Note1: before this file is required, it is necessary to require the correspondent 'verify.jinc':
//        - for 'ref' implementations: crypto_kem/kyber/common/amd64/ref/verify.jinc
//        - for 'avx2' implementations: crypto_kem/kyber/common/amd64/avx2/verify.jinc
//
// Note2: due to the integration of hakyber implementations into libjade, this file is no longer
//        required by kyber768/amd64/ref/

#[returnaddress="stack"]
fn __crypto_kem_keypair_derand_jazz(reg u64 pkp, reg u64 skp, reg ptr u8[2*KYBER_SYMBYTES] coins, reg u64 ms) -> #msf reg u64
{
  stack u8[32] h_pk;
  stack u64 s_skp s_pkp;
  stack ptr u8[2*KYBER_SYMBYTES] s_coins;
  reg u64 t64;
  inline int i;

  s_coins = coins;
  s_pkp = pkp;
  s_skp = skp;

  ms = __indcpa_keypair_derand(pkp, skp, coins[0:32], ms);

  skp = s_skp;
  skp = #protect(skp, ms);
  skp += KYBER_POLYVECBYTES;
  pkp = s_pkp;
  pkp = #protect(pkp, ms);

  for i=0 to KYBER_INDCPA_PUBLICKEYBYTES/8
  {
    t64 = (u64)[pkp + 8*i];
    (u64)[skp] = t64;
    skp += 8;
  }

  s_skp = skp;
  pkp = s_pkp;
  pkp = #protect(pkp, ms);
  t64 = KYBER_INDCPA_PUBLICKEYBYTES;
  h_pk, ms = _sha3_256(h_pk, pkp, t64, ms);
  skp = s_skp;
  coins = s_coins;

  skp = #protect(skp, ms);

  reg ptr u8[32] ph_pk;
  ph_pk = h_pk;
  ph_pk = #protect_ptr(ph_pk, ms);

  __fromstack32u8(skp, ph_pk);
  skp += 32;

  reg ptr u8[32] pcoins;
  pcoins = coins[32:32];
  pcoins = #protect_ptr(pcoins, ms);

  __fromstack32u8(skp, pcoins);

  return ms;
}

#[returnaddress="stack"]
fn __crypto_kem_enc_derand_jazz(reg u64 ctp, reg u64 shkp, reg u64 pkp, reg ptr u8[KYBER_SYMBYTES] coins, reg u64 ms) -> #msf reg u64
{
  stack u8[KYBER_SYMBYTES * 2] buf kr;
  stack u64 s_pkp s_ctp s_shkp;
  reg u64 t64;
  inline int i;

  s_pkp = pkp;
  s_ctp = ctp;
  s_shkp = shkp;

  buf[0:32], ms = _sha3_256_32(buf[0:32], coins, ms);

  pkp = s_pkp;
  pkp = #protect(pkp, ms);

  t64 = KYBER_PUBLICKEYBYTES;
  buf[KYBER_SYMBYTES:32], ms = _sha3_256(buf[KYBER_SYMBYTES:32], pkp, t64, ms);

  kr, ms = _sha3_512_64(kr, buf, ms);

  pkp = s_pkp;
  pkp = #protect(pkp, ms);

  ms = __indcpa_enc_0(s_ctp, buf[0:KYBER_INDCPA_MSGBYTES], pkp, kr[KYBER_SYMBYTES:KYBER_SYMBYTES], ms);

  ctp = s_ctp;
  ctp = #protect(ctp, ms);
  t64 = KYBER_INDCPA_BYTES;
  kr[KYBER_SYMBYTES:32], ms = _sha3_256(kr[KYBER_SYMBYTES:32], ctp, t64, ms);

  shkp = s_shkp;
  shkp = #protect(shkp, ms);
  t64 = KYBER_SSBYTES;
  ms = _shake256_64(shkp, t64, kr, ms);

  return ms;
}

inline
fn __crypto_kem_dec_jazz(reg u64 shkp, reg u64 ctp, reg u64 skp, reg u64 ms) -> #msf reg u64
{
  stack u8[KYBER_INDCPA_BYTES] ctpc;
  stack u8[2*KYBER_SYMBYTES] kr buf;
  stack u64 s_skp s_ctp s_shkp;
  reg u64 pkp hp zp t64 cnd;
  inline int i;

  s_shkp = shkp;
  s_ctp = ctp;
  skp = skp;

  buf[0:KYBER_INDCPA_MSGBYTES], ms = __indcpa_dec(buf[0:KYBER_INDCPA_MSGBYTES], ctp, skp, ms);

  #mmx reg u64 s_ms;
  s_ms = #mov_msf(ms);

  hp = #LEA(skp + 32); //hp = skp + 32;
  hp += 24 * KYBER_K * KYBER_N>>3;

  for i=0 to KYBER_SYMBYTES/8
  {
    t64 = (u64)[hp + 8*i];
    buf.[u64 KYBER_SYMBYTES + 8*i] = t64;
  }

  s_skp = skp;
  ms = #mov_msf(s_ms);

  kr, ms = _sha3_512_64(kr, buf, ms);

  pkp = s_skp;
  pkp = #protect(pkp, ms);
  pkp += 12 * KYBER_K * KYBER_N>>3;

  ctpc, ms = __indcpa_enc_1(ctpc, buf[0:KYBER_INDCPA_MSGBYTES], pkp, kr[KYBER_SYMBYTES:KYBER_SYMBYTES], ms);

  ctp = s_ctp;
  ctp = #protect(ctp, ms);
  cnd = __verify(ctp, ctpc);

  zp = s_skp;
  zp = #protect(zp, ms);
  zp += 64;
  zp += 24 * KYBER_K * KYBER_N>>3;
  kr[0:KYBER_SYMBYTES] = __cmov(kr[0:KYBER_SYMBYTES], zp, cnd);

  t64 = KYBER_INDCPA_BYTES;
  kr[KYBER_SYMBYTES:32], ms = _sha3_256(kr[KYBER_SYMBYTES:32], ctp, t64, ms);

  shkp = s_shkp;
  shkp = #protect(shkp, ms);
  t64 = KYBER_SSBYTES;
  ms = _shake256_64(shkp, t64, kr, ms);

  return ms;
}
